{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the code for 8 queen Dynamic programming problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the required libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# board state memory\n",
    "board_state_memory:dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "\n",
    "# creating an N * N board\n",
    "board = np.zeros((N, N), np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create board string\n",
    "def create_board_string(board):\n",
    "    board_string = ''\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            board_string += str(board[i][j])\n",
    "    return board_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000000000000000000000000000000000000000000000000000000000000000'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the Create_board_string function\n",
    "create_board_string(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the board and positioning the queen at 1 column, 1 row\n",
    "board_copy = board.copy()\n",
    "board_copy[0, 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0100000000000000000000000000000000000000000000000000000000000000'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board_copy\n",
    "create_board_string(board_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_board_safe(board):\n",
    "    \n",
    "    #This is where the dynaic programming comes into picture. Where we use the board_state_memory to cache.\n",
    "    board_key = create_board_string(board)\n",
    "    \n",
    "    if board_key in board_state_memory:\n",
    "        print('Using Cached Information')\n",
    "        return board_state_memory[board_key]\n",
    "    \n",
    "    # calculate the sum in every row of the board\n",
    "    row_sum = np.sum(board, axis = 1)\n",
    "    # checking if a row has more than 1 queen - we will check the sum or elements in a row\n",
    "    if len(row_sum[np.where(row_sum > 1)]) > 0:\n",
    "        board_state_memory[board_key] = False\n",
    "        return False\n",
    "    \n",
    "    # checking column sum in every row of the board\n",
    "    col_sum = np.sum(board, axis = 0)\n",
    "    if len(col_sum[np.where(col_sum > 1)]) > 0:\n",
    "        board_state_memory[board_key] = False\n",
    "        return False\n",
    "    \n",
    "    diags = [board[::-1,:].diagonal(i) for i in range(-board.shape[0] + 1, board.shape[1])]\n",
    "    \n",
    "    diags.extend(board.diagonal(i) for i in range(board.shape[1] - 1, -board.shape[0], -1))\n",
    "    \n",
    "    for diag in diags:\n",
    "        if np.sum(diag) > 1:\n",
    "            board_state_memory[board_key] = False\n",
    "            return False\n",
    "        \n",
    "    board_state_memory[board_key] = True\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_copy = board.copy()\n",
    "board_copy[1][0] = 1\n",
    "board_copy[2][3] = 1\n",
    "board_copy[0][1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(board_copy)\n",
    "is_board_safe(board_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the placing of the Queen \n",
    "\n",
    "def place_queen_safely(board, column):\n",
    "    \n",
    "    if column >= N:\n",
    "        # All N queens are placed correctly\n",
    "        return True\n",
    "    \n",
    "    for row in range(N):\n",
    "        board[row][column] = 1\n",
    "        \n",
    "        safe = False\n",
    "        \n",
    "        if is_board_safe(board):\n",
    "            safe = place_queen_safely(board, column + 1)\n",
    "    \n",
    "        if not safe:\n",
    "            board[row][column] = 0\n",
    "            \n",
    "        else: \n",
    "            break\n",
    "    return safe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0100000010000000000100000000000000000000000000000000000000000000': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board_state_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "board = np.zeros((N,N), np.int8)\n",
    "placed = place_queen_safely(board, 0)\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placed\n",
    "# just checking if everything is set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Q-learning and SARSA in Reinforcement Learning.\n",
    "Q-learning gives us the optimal policy to maximize the returns in a Reinforcement learning.\n",
    "\n",
    "Agent observes the environment and takes actions based on a policy.\n",
    "The output of reinforcement learning is a set of actions which maximize the cumulative rewards.\n",
    "\n",
    "Policy Search algorithms\n",
    "1. Brute Force methods\n",
    "2. Policy Gradient methods\n",
    "3. Value Function metods\n",
    "\n",
    "Value Function methods are most common\n",
    "\n",
    "Q-learning techniques (Temporal difference)\n",
    "SARSA method (Another implementation of Q-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Case is a student deciding whether to enroll in further studies or start the first job.\n",
    "Agent here is the person (an undergraduate)\n",
    "Reward is the cumulative savings in money by the age of 60 years.\n",
    "Policy is the algorithm that chooses either of the two steps(enroll in further studies or Starting first jobs) that gives maximum rewards.\n",
    "\n",
    "Any state could be define as follows\n",
    "(<State of the plan to further studies>,<Years of Experience in the job>)\n",
    "    \n",
    "Initial State is (BS-EE, 0) # Bachelors in Electrical Engineering, 0 years of work experience\n",
    "If Action = further Studies\n",
    "    Next State = (MS-CS, 0) # Masters in Computers, 0 years of work experience\n",
    "Else:\n",
    "    Next State = (BS-EE, 2) # If she/he didn't choose further studies, then 2 years of work experience will be earned.\n",
    "    \n",
    "# Q-Learning is a Value Function policy\n",
    "# Estimate rewards at the Terminal (Destination, Final positions) and probabilities of getting there, then work backwards.\n",
    "\n",
    "Once the total path is ready, we calculate the probabilities and rewards at each state. Also, the penalty i.e., the cost of education is to be considered.\n",
    "\n",
    "At the first step, when we calculate rewards - penalties, we end up with the amounts which determine the immediate decision to take.\n",
    "\n",
    "# Q-Learning and SARSA: Maximize the sum of immediate reward and discounted future reward.\n",
    "\n",
    "Steps in implementing a Q-Learning\n",
    "1. Set up a Q-Table for every state-action combination. Rows form the states and Columns form the actions.\n",
    "2. Q-value is the estimate of optimal state value. Q-Value helps the agent in taking decisions.\n",
    "3. What is a Q-value: If we take an action from State S, the cumulative long run reward is called as Q-Value.\n",
    "   it is represented by Q(s, a)\n",
    "   At a particular state, we choose the maximum Q-Value to take a particular action, as it maximizes the reward.\n",
    "   \n",
    "# Different methods can be used to calculate the Q-Values.\n",
    "# Temporal difference and SARSA are the different ways in which the Q-Values can be calculated. Temporal difference is nothing but Q-Learning. SARSA is another verion of it.\n",
    "\n",
    "# Temporal Difference Notes - Refer image.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
